{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6bdaa76-1286-491c-95eb-846625867a1f",
   "metadata": {},
   "source": [
    "**ðŸ”’ Proprietary & All Rights Reserved**\n",
    "\n",
    "**Â© 2025 Sweety Seelam.** This work is proprietary and protected by copyright. All content, models, code, and visuals are Â© 2025 Sweety Seelam. \n",
    "No part of this project, app, code, or analysis may be copied, reproduced, distributed, or used for any purposeâ€”commercial or otherwiseâ€”without explicit written permission from the author.\n",
    "\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ba4ef-484c-4929-849e-f0a2c5e13d68",
   "metadata": {},
   "source": [
    "# StreamIntel360: A Multi-Agent RAG Platform for Streaming Content & Revenue Intelligence\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4cec0c",
   "metadata": {},
   "source": [
    "# 02 â€“ Embedding Experiments & Similarity Search\n",
    "\n",
    "This notebook explores:\n",
    "\n",
    "- How to construct text representations from the Netflix titles catalog.\n",
    "- How to embed titles using SentenceTransformers.\n",
    "- How to build a FAISS index.\n",
    "- How to run example similarity queries (e.g., given a concept or logline).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c71eea7b-83cb-488c-9836-0f0d1b7988f3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (1.13.1)\n",
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.20.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from sentence-transformers) (4.56.0.dev0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from sentence-transformers) (2.2.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Collecting tensorflow<2.21,>=2.20 (from tf-keras)\n",
      "  Downloading tensorflow-2.20.0-cp312-cp312-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow<2.21,>=2.20->tf-keras)\n",
      "  Downloading protobuf-6.33.2-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.73.1)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow<2.21,>=2.20->tf-keras)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.10.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sweet\\.anaconda\\download\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\n",
      "Downloading tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 15.2 MB/s eta 0:00:00\n",
      "Downloading tensorflow-2.20.0-cp312-cp312-win_amd64.whl (331.9 MB)\n",
      "   ---------------------------------------- 0.0/331.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 10.5/331.9 MB 54.7 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 22.0/331.9 MB 53.6 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 32.8/331.9 MB 53.4 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 43.5/331.9 MB 52.2 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 53.7/331.9 MB 51.1 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 64.5/331.9 MB 51.4 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 75.2/331.9 MB 51.0 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 87.6/331.9 MB 51.7 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 99.6/331.9 MB 52.1 MB/s eta 0:00:05\n",
      "   ------------- ------------------------- 112.2/331.9 MB 52.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------ 125.3/331.9 MB 53.3 MB/s eta 0:00:04\n",
      "   ---------------- ---------------------- 138.7/331.9 MB 54.0 MB/s eta 0:00:04\n",
      "   ----------------- --------------------- 150.7/331.9 MB 54.1 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 162.3/331.9 MB 54.3 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 175.1/331.9 MB 54.3 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 188.0/331.9 MB 54.8 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 197.7/331.9 MB 54.2 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 211.0/331.9 MB 54.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 222.3/331.9 MB 54.4 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 234.1/331.9 MB 54.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 248.0/331.9 MB 55.1 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 261.6/331.9 MB 55.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 273.9/331.9 MB 55.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 287.6/331.9 MB 56.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 302.3/331.9 MB 56.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 313.5/331.9 MB 57.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  326.4/331.9 MB 57.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.9/331.9 MB 57.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.9/331.9 MB 57.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.9/331.9 MB 57.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.9/331.9 MB 57.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.9/331.9 MB 57.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.9/331.9 MB 57.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  331.9/331.9 MB 57.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 331.9/331.9 MB 43.8 MB/s eta 0:00:00\n",
      "Downloading protobuf-6.33.2-cp310-abi3-win_amd64.whl (436 kB)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.5/5.5 MB 37.3 MB/s eta 0:00:00\n",
      "Installing collected packages: protobuf, tensorboard, tensorflow, tf-keras\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.19.0\n",
      "    Uninstalling tensorboard-2.19.0:\n",
      "      Successfully uninstalled tensorboard-2.19.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.19.0\n",
      "    Uninstalling tensorflow-2.19.0:\n",
      "      Successfully uninstalled tensorflow-2.19.0\n",
      "Successfully installed protobuf-6.33.2 tensorboard-2.20.0 tensorflow-2.20.0 tf-keras-2.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.37.1 requires protobuf<6,>=3.20, but you have protobuf 6.33.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - Install dependencies\n",
    "!pip install sentence-transformers faiss-cpu tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b3cbc2a-81d7-485e-8325-8f736a9b5a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.20.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tf_keras\n",
    "tf_keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d145caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sweet\\.anaconda\\download\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(WindowsPath('../data/raw/netflix_titles.csv'), True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2 â€“ Code: Imports & Paths\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "DATA_DIR = Path(\"..\") / \"data\" / \"raw\"\n",
    "FILE_PATH = DATA_DIR / \"netflix_titles.csv\"\n",
    "\n",
    "FILE_PATH, FILE_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "892a55e9-f81d-4340-b385-362f42f58011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_id</th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>director</th>\n",
       "      <th>cast</th>\n",
       "      <th>country</th>\n",
       "      <th>date_added</th>\n",
       "      <th>release_year</th>\n",
       "      <th>rating</th>\n",
       "      <th>duration</th>\n",
       "      <th>listed_in</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s1</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Dick Johnson Is Dead</td>\n",
       "      <td>Kirsten Johnson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>September 25, 2021</td>\n",
       "      <td>2020</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>90 min</td>\n",
       "      <td>Documentaries</td>\n",
       "      <td>As her father nears the end of his life, filmm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s2</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>Blood &amp; Water</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ama Qamata, Khosi Ngema, Gail Mabalane, Thaban...</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>September 24, 2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>2 Seasons</td>\n",
       "      <td>International TV Shows, TV Dramas, TV Mysteries</td>\n",
       "      <td>After crossing paths at a party, a Cape Town t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>Ganglands</td>\n",
       "      <td>Julien Leclercq</td>\n",
       "      <td>Sami Bouajila, Tracy Gotoas, Samuel Jouy, Nabi...</td>\n",
       "      <td></td>\n",
       "      <td>September 24, 2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>1 Season</td>\n",
       "      <td>Crime TV Shows, International TV Shows, TV Act...</td>\n",
       "      <td>To protect his family from a powerful drug lor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s4</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>Jailbirds New Orleans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>September 24, 2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>1 Season</td>\n",
       "      <td>Docuseries, Reality TV</td>\n",
       "      <td>Feuds, flirtations and toilet talk go down amo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s5</td>\n",
       "      <td>TV Show</td>\n",
       "      <td>Kota Factory</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mayur More, Jitendra Kumar, Ranjan Raj, Alam K...</td>\n",
       "      <td>India</td>\n",
       "      <td>September 24, 2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>TV-MA</td>\n",
       "      <td>2 Seasons</td>\n",
       "      <td>International TV Shows, Romantic TV Shows, TV ...</td>\n",
       "      <td>In a city of coaching centers known to train I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  show_id     type                  title         director  \\\n",
       "0      s1    Movie   Dick Johnson Is Dead  Kirsten Johnson   \n",
       "1      s2  TV Show          Blood & Water              NaN   \n",
       "2      s3  TV Show              Ganglands  Julien Leclercq   \n",
       "3      s4  TV Show  Jailbirds New Orleans              NaN   \n",
       "4      s5  TV Show           Kota Factory              NaN   \n",
       "\n",
       "                                                cast        country  \\\n",
       "0                                                NaN  United States   \n",
       "1  Ama Qamata, Khosi Ngema, Gail Mabalane, Thaban...   South Africa   \n",
       "2  Sami Bouajila, Tracy Gotoas, Samuel Jouy, Nabi...                  \n",
       "3                                                NaN                  \n",
       "4  Mayur More, Jitendra Kumar, Ranjan Raj, Alam K...          India   \n",
       "\n",
       "           date_added  release_year rating   duration  \\\n",
       "0  September 25, 2021          2020  PG-13     90 min   \n",
       "1  September 24, 2021          2021  TV-MA  2 Seasons   \n",
       "2  September 24, 2021          2021  TV-MA   1 Season   \n",
       "3  September 24, 2021          2021  TV-MA   1 Season   \n",
       "4  September 24, 2021          2021  TV-MA  2 Seasons   \n",
       "\n",
       "                                           listed_in  \\\n",
       "0                                      Documentaries   \n",
       "1    International TV Shows, TV Dramas, TV Mysteries   \n",
       "2  Crime TV Shows, International TV Shows, TV Act...   \n",
       "3                             Docuseries, Reality TV   \n",
       "4  International TV Shows, Romantic TV Shows, TV ...   \n",
       "\n",
       "                                         description  \n",
       "0  As her father nears the end of his life, filmm...  \n",
       "1  After crossing paths at a party, a Cape Town t...  \n",
       "2  To protect his family from a powerful drug lor...  \n",
       "3  Feuds, flirtations and toilet talk go down amo...  \n",
       "4  In a city of coaching centers known to train I...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3 â€“ Code: Load Data & Build Basic Corpus (clean version)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "cols_to_use = [\n",
    "    \"show_id\",\n",
    "    \"type\",\n",
    "    \"title\",\n",
    "    \"director\",\n",
    "    \"cast\",\n",
    "    \"country\",\n",
    "    \"date_added\",\n",
    "    \"release_year\",\n",
    "    \"rating\",\n",
    "    \"duration\",\n",
    "    \"listed_in\",\n",
    "    \"description\",\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\n",
    "    FILE_PATH,\n",
    "    encoding=\"latin-1\",\n",
    "    usecols=cols_to_use,   # <- only keep the real columns\n",
    ")\n",
    "\n",
    "df = df.copy()\n",
    "\n",
    "# Ensure key columns exist\n",
    "for col in [\"title\", \"description\", \"listed_in\", \"type\", \"country\", \"release_year\"]:\n",
    "    if col not in df.columns:\n",
    "        print(f\"WARNING: Column {col} not found in dataset\")\n",
    "\n",
    "df[\"title\"] = df[\"title\"].astype(str).str.strip()\n",
    "df[\"description\"] = df[\"description\"].fillna(\"\").astype(str)\n",
    "df[\"listed_in\"] = df[\"listed_in\"].fillna(\"\").astype(str)\n",
    "df[\"country\"] = df[\"country\"].fillna(\"\").astype(str)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8969b67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Title: Dick Johnson Is Dead | Type: Movie | Ge...\n",
       "1    Title: Blood & Water | Type: TV Show | Genres:...\n",
       "2    Title: Ganglands | Type: TV Show | Genres: Cri...\n",
       "3    Title: Jailbirds New Orleans | Type: TV Show |...\n",
       "4    Title: Kota Factory | Type: TV Show | Genres: ...\n",
       "Name: corpus_text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4 â€“ Code: Build Text Variants\n",
    "def build_text_variant(row, variant: str = \"baseline\") -> str:\n",
    "    \"\"\"Build different corpus variants to test which gives better semantic search.\"\"\"\n",
    "    if variant == \"title_only\":\n",
    "        return row[\"title\"]\n",
    "\n",
    "    if variant == \"title_description\":\n",
    "        return f\"Title: {row['title']} | Description: {row['description']}\"\n",
    "\n",
    "    # baseline: include genres and country\n",
    "    if variant == \"baseline\":\n",
    "        parts = [\n",
    "            f\"Title: {row['title']}\",\n",
    "            f\"Type: {row.get('type', '')}\",\n",
    "            f\"Genres: {row.get('listed_in', '')}\",\n",
    "            f\"Country: {row.get('country', '')}\",\n",
    "            f\"Year: {row.get('release_year', '')}\",\n",
    "            f\"Description: {row['description']}\",\n",
    "        ]\n",
    "        return \" | \".join([p for p in parts if p])\n",
    "\n",
    "    raise ValueError(f\"Unknown variant: {variant}\")\n",
    "\n",
    "VARIANT = \"baseline\"  # change this to try \"title_only\" or \"title_description\"\n",
    "\n",
    "df[\"corpus_text\"] = df.apply(build_text_variant, axis=1, variant=VARIANT)\n",
    "df[\"corpus_text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf12a38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59caaba41554e7481cfeef4f873ab31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sweet\\.anaconda\\download\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sweet\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64823ddf9d548fdaa5bd109cbb9b18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de75c75ddd53421bac1f39811adc6410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a14ecbb06e47bc9dcc18c924ce4d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581b4d19114a4fdd84fc762faa5af138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014183fe7c634fd8a3a6acbc712609a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e219b5e49e994d33a535e9772ebb0207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0fdabb03234229ad09f6f5fe8e38ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482ad28b108643c49e375dfd7020a778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd3ef6f8bf3450c9d96287a0884c10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SentenceTransformer model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 â€“ Code: Load Embedding Model\n",
    "# A popular all-purpose model; you can choose others later.\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "embed_model = SentenceTransformer(model_name)\n",
    "\n",
    "print(f\"Loaded SentenceTransformer model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7aa9079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc6b500b1a54a82a082715f8c6942ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(8809, 384)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 6 â€“ Code: Compute Embeddings\n",
    "texts = df[\"corpus_text\"].tolist()\n",
    "len(texts)\n",
    "\n",
    "# WARNING: This can take some time if dataset is large.\n",
    "embeddings = embed_model.encode(texts, batch_size=64, show_progress_bar=True)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e19dc77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index size: 8809\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 â€“ Code: Build FAISS Index\n",
    "d = embeddings.shape[1]  # embedding dimensionality\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(embeddings)\n",
    "\n",
    "print(\"Index size:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31b05c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 â€“ Helper: Search Function\n",
    "def search_similar(query: str, k: int = 5):\n",
    "    \"\"\"Encode a query and return top-k similar titles with scores.\"\"\"\n",
    "    q_emb = embed_model.encode([query])\n",
    "    distances, indices = index.search(q_emb, k)\n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        row = df.iloc[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"rank\": len(results) + 1,\n",
    "                \"title\": row[\"title\"],\n",
    "                \"type\": row.get(\"type\", \"\"),\n",
    "                \"distance\": float(dist),\n",
    "                \"description\": row[\"description\"],\n",
    "                \"genres\": row[\"listed_in\"],\n",
    "                \"year\": row[\"release_year\"],\n",
    "            }\n",
    "        )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76496a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'title': 'Dark Crimes',\n",
       "  'type': 'Movie',\n",
       "  'distance': 0.5850995182991028,\n",
       "  'description': 'A detective on a cold murder case discovers that a famous writerÃ¢\\x80\\x99s latest novel contains details chillingly similar to the crime heÃ¢\\x80\\x99s investigating.',\n",
       "  'genres': 'Dramas, Thrillers',\n",
       "  'year': 2016},\n",
       " {'rank': 2,\n",
       "  'title': 'Small Town Crime',\n",
       "  'type': 'Movie',\n",
       "  'distance': 0.5972224473953247,\n",
       "  'description': \"When a disgraced ex-cop discovers a dying woman, he's compelled to track down her killer Ã¢\\x80\\x94 an act of self-redemption that takes him down a dark path.\",\n",
       "  'genres': 'Thrillers',\n",
       "  'year': 2017},\n",
       " {'rank': 3,\n",
       "  'title': 'November Criminals',\n",
       "  'type': 'Movie',\n",
       "  'distance': 0.7340857982635498,\n",
       "  'description': \"Dissatisfied with the policeÃ¢\\x80\\x99s progress, a teen and his best friend investigate their classmate's murder, treading down a dark path to the uneasy truth.\",\n",
       "  'genres': 'Dramas, Thrillers',\n",
       "  'year': 2017},\n",
       " {'rank': 4,\n",
       "  'title': 'Night Stalker: The Hunt for a Serial Killer',\n",
       "  'type': 'TV Show',\n",
       "  'distance': 0.7427600622177124,\n",
       "  'description': \"Beneath the sunlit glamour of 1985 LA lurks a relentlessly evil serial killer. In this true-crime story, two detectives won't rest until they catch him.\",\n",
       "  'genres': 'Crime TV Shows, Docuseries',\n",
       "  'year': 2021},\n",
       " {'rank': 5,\n",
       "  'title': 'Twin Murders: the Silence of the White City',\n",
       "  'type': 'Movie',\n",
       "  'distance': 0.8043779134750366,\n",
       "  'description': \"A detective returns to Vitoria-Gasteiz to solve murders mimicking those allegedly committed by a serial killer who's about to be released from prison.\",\n",
       "  'genres': 'International Movies, Thrillers',\n",
       "  'year': 2020}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 9 â€“ Test Query 1\n",
    "query_1 = \"A dark crime thriller about a serial killer in a big city\"\n",
    "results_1 = search_similar(query_1, k=5)\n",
    "results_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c56ed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1 | Dark Crimes (2016) [Movie]\n",
      "Genres: Dramas, Thrillers\n",
      "Distance: 0.5851\n",
      "Desc: A detective on a cold murder case discovers that a famous writerÃ¢Â€Â™s latest novel contains details chillingly similar to the crime heÃ¢Â€Â™s investigating....\n",
      "--------------------------------------------------------------------------------\n",
      "#2 | Small Town Crime (2017) [Movie]\n",
      "Genres: Thrillers\n",
      "Distance: 0.5972\n",
      "Desc: When a disgraced ex-cop discovers a dying woman, he's compelled to track down her killer Ã¢Â€Â” an act of self-redemption that takes him down a dark path....\n",
      "--------------------------------------------------------------------------------\n",
      "#3 | November Criminals (2017) [Movie]\n",
      "Genres: Dramas, Thrillers\n",
      "Distance: 0.7341\n",
      "Desc: Dissatisfied with the policeÃ¢Â€Â™s progress, a teen and his best friend investigate their classmate's murder, treading down a dark path to the uneasy truth....\n",
      "--------------------------------------------------------------------------------\n",
      "#4 | Night Stalker: The Hunt for a Serial Killer (2021) [TV Show]\n",
      "Genres: Crime TV Shows, Docuseries\n",
      "Distance: 0.7428\n",
      "Desc: Beneath the sunlit glamour of 1985 LA lurks a relentlessly evil serial killer. In this true-crime story, two detectives won't rest until they catch him....\n",
      "--------------------------------------------------------------------------------\n",
      "#5 | Twin Murders: the Silence of the White City (2020) [Movie]\n",
      "Genres: International Movies, Thrillers\n",
      "Distance: 0.8044\n",
      "Desc: A detective returns to Vitoria-Gasteiz to solve murders mimicking those allegedly committed by a serial killer who's about to be released from prison....\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Then we can format the results\n",
    "for r in results_1:\n",
    "    print(f\"#{r['rank']} | {r['title']} ({r['year']}) [{r['type']}]\")\n",
    "    print(f\"Genres: {r['genres']}\")\n",
    "    print(f\"Distance: {r['distance']:.4f}\")\n",
    "    print(f\"Desc: {r['description'][:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20ce6cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1 | Benji (2018) [Movie]\n",
      "Genres: Children & Family Movies, Dramas\n",
      "Distance: 0.8095\n",
      "Desc: A determined dog comes to the rescue and helps heal a broken family when a young boy and his sister stumble into some serious danger....\n",
      "--------------------------------------------------------------------------------\n",
      "#2 | Show Dogs (2018) [Movie]\n",
      "Genres: Children & Family Movies, Comedies\n",
      "Distance: 0.8127\n",
      "Desc: A rough and tough police dog must go undercover with an FBI agent as a prim and proper pet at a dog show to save a baby panda from an illegal sale....\n",
      "--------------------------------------------------------------------------------\n",
      "#3 | Dog Gone Trouble (2021) [Movie]\n",
      "Genres: Children & Family Movies, Comedies\n",
      "Distance: 0.8244\n",
      "Desc: The privileged life of a pampered dog named Trouble is turned upside-down when he gets lost and must learn to survive on the big-city streets....\n",
      "--------------------------------------------------------------------------------\n",
      "#4 | All Dogs Go to Heaven (1989) [Movie]\n",
      "Genres: Children & Family Movies\n",
      "Distance: 0.8393\n",
      "Desc: When a canine con artist becomes an angel, he sneaks back to Earth and crosses paths with an orphan girl who can speak to animals....\n",
      "--------------------------------------------------------------------------------\n",
      "#5 | Life in the Doghouse (2018) [Movie]\n",
      "Genres: Documentaries, LGBTQ Movies\n",
      "Distance: 0.8424\n",
      "Desc: A couple operates a bustling dog rescue out of their own home, vowing to give a safe space to the neediest pups on the planet Ã¢Â€Â“ 10,000 and counting....\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 â€“ Test Query 2\n",
    "query_2 = \"Feel-good family movie about a dog and children\"\n",
    "results_2 = search_similar(query_2, k=5)\n",
    "\n",
    "for r in results_2:\n",
    "    print(f\"#{r['rank']} | {r['title']} ({r['year']}) [{r['type']}]\")\n",
    "    print(f\"Genres: {r['genres']}\")\n",
    "    print(f\"Distance: {r['distance']:.4f}\")\n",
    "    print(f\"Desc: {r['description'][:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618b6507",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Observations\n",
    "\n",
    "- The `baseline` text variant (Title + Type + Genres + Country + Year + Description) provides richer context for semantic search.\n",
    "- Queries like \"dark crime thriller\" or \"family movie about a dog\" return reasonably related titles.\n",
    "- These experiments inform how we design the RAG corpus used by the StreamIntel360 backend:\n",
    "  - We will likely use a similar combined text structure for embeddings.\n",
    "  - We know roughly how many neighbors (`k`) give meaningful results (e.g., 5â€“10).\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d701a4b-3870-465b-9be9-61393151f799",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbaf600-8270-4b87-8553-0874c3e8cde6",
   "metadata": {},
   "source": [
    "***â€œHow should we turn titles into vectors for semantic search?â€***\n",
    "\n",
    "**What I did & why?**\n",
    "\n",
    "**1.Loaded the same curated columns from netflix_titles.csv.**\n",
    "\n",
    "- Reason: keep corpus construction consistent with Notebook 1 and the backend.\n",
    "\n",
    "**2.Built corpus_text with build_text_variant:**\n",
    "\n",
    "- Variants: \"title_only\", \"title_description\", \"baseline\" (title + type + genres + country + year + description).\n",
    "\n",
    "- Reason: experiment with different text recipes to see which gives the best semantic behavior.\n",
    "\n",
    "**3.Loaded SentenceTransformer(\"all-MiniLM-L6-v2\").**\n",
    "\n",
    "- Reason: a lightweight, free, GPU/CPU-friendly model that works well for semantic search without API keys.\n",
    "\n",
    "**4.Encoded all 8,809 titles into embeddings.**\n",
    "\n",
    "- Reason: obtain a dense vector representation for each title that captures semantic meaning.\n",
    "\n",
    "**5.Built a FAISS index (IndexFlatL2) and added all embeddings.**\n",
    "\n",
    "- Reason: enable fast nearest-neighbor search over the catalog.\n",
    "\n",
    "**6.Defined search_similar(query, k) and ran natural queries:**\n",
    "\n",
    "- â€œdark crime thriller about a serial killerâ€ â†’ crime thrillers and serial-killer content.\n",
    "\n",
    "- â€œfeel-good family movie about a dog and childrenâ€ â†’ Benji, Show Dogs, Dog Gone Trouble, etc.\n",
    "\n",
    "- Reason: qualitatively validate â€œdoes this text recipe + model give intuitive neighbors?â€\n",
    "\n",
    "**What have I achieved?**\n",
    "\n",
    "- I validated that the baseline corpus design works: rich context (title + type + genres + country + year + description) produces semantically meaningful neighbors.\n",
    "\n",
    "- I have confirmed FAISS + MiniLM can serve as the RAG retrieval layer for StreamIntel360 on the laptop.\n",
    "\n",
    "- Now we have a reference implementation of: corpus building â†’ embeddings â†’ FAISS index â†’ similarity search.\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec719df9-c42a-469c-9e8f-d1cf36d8660b",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7226a0ac-ce92-4b2c-85b1-ab4983d4e0b9",
   "metadata": {},
   "source": [
    "- This notebook therefore establishes the core RAG retrieval engine for StreamIntel360, which the multi-agent backend will use to ground its analysis, find comparable titles, and support executive-style recommendations.\n",
    "\n",
    "- Transformed that catalog into a searchable semantic space using SentenceTransformers and FAISS.\n",
    "\n",
    "- Constructed a rich corpus_text per title that blends title, type, genre, country, year, and description into one representation.\n",
    "\n",
    "- Generated embeddings for all 8,809 titles and built a FAISS index that can return the most similar titles in milliseconds.\n",
    "\n",
    "- Test queries showed that the system retrieves highly relevant and thematically aligned titles, matching human expectations.\n",
    "\n",
    "- This notebook proves that the RAG core (retrieval) is sane and effective before wiring it into any backend or UI.\n",
    "\n",
    "- It answers: â€œIf I describe a concept in natural language, can we surface reasonable titles from the catalog?â€\n",
    "\n",
    "- Yes: the results for both crime thriller and dog-family queries show that this design is good enough to power the AI copilot.\n",
    "\n",
    "- Practically, it is both a research sandbox and a clean, documented recipe that the backend can mirror."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
